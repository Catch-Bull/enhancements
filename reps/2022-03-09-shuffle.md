## Summary
### General Motivation

Shuffle is an important primitive for data processing, e.g., sort, and ML ingest workloads, e.g., shuffling training data.
Currently, Ray offers shuffle through the Datasets library, but the implementation has known data scalability and performance limitations past 10TB data scales.
The goal of this proposal is to improve Datasets shuffle stability and scalability, through both Datasets and Ray core improvements.
By the end of this work, we hope to achieve petabyte-scale shuffle operations with Datasets.

### Should this change be within `ray` or outside?

These changes would lie within Ray Datasets, to improve the shuffle algorithm, and within Ray core, to improve the shuffle execution.

## Stewardship
### Required Reviewers

@stephanie-wang, @ericl, @scv119

### Shepherd of the Proposal (should be a senior committer)

@ericl

## Design and Architecture

Currently Datasets shuffle is implemented as a map and reduce stage of Ray tasks. For example:

```python
@ray.remote
def map(partition, split_fn):
    return [block for block in split_fn(partition)]

@ray.remote
def reduce(*partitions):
    return merge(partitions)

map_outputs = [map.options(num_returns=num_reducers).remote(partition, split_fn) for partition in partitions]
reduce_outputs = [reduce.remote(*[map_output[i] for map_output in map_outputs]) for i in range(num_reducers)]
ray.get(reduce_outputs)
```

This forms a task graph that looks something like this:

![MapReduce](https://miro.medium.com/max/680/1*nJYIs2ktVkqVsgSUCzfjaA.gif)

The number of intermediate map outputs increases quadratically with the number of partitions and therefore the dataset size. This has two primary problems:
1. I/O efficiency worsens as the data size gets larger and the size of each intermediate map output gets smaller.
2. The system overhead of each map output becomes a scalability bottleneck.

We propose addressing (1) through improvements in the Datasets library and (2) through Ray core improvements.

### Ray Datasets

To improve I/O efficiency, we will incorporate some of the work done in the [Exoshuffle paper](https://arxiv.org/abs/2203.05072) on improving shuffle performance on Ray.
Exoshuffle implements a "push-based shuffle" using Ray tasks, which pushes map outputs directly to their reducer while the map stage is still executing.
More details on push-based shuffle can be found in the [Magnet paper](https://dl.acm.org/doi/10.14778/3415478.3415558), which implemented the algorithm as a part of Spark.

In the current Datasets shuffle, the metadata overhead for `ObjectRefs` in Ray becomes a bottleneck at around 10TB or more.
The Exoshuffle work reduces the total number of `ObjectRefs` and showed that it is possible to run sort at 100TB or more data scales on Ray.
To incorporate this work, we will:
1. Unify Datasets shuffle-based primitives on the same internal shuffle implementation.
2. Benchmark Datasets shuffle-based primitives.
3. Implement a push-based shuffle in Datasets.

### Ray core

Although push-based shuffle reduces the amount of system metadata, it is not enough to scale to petabyte-size data.
Thus, we also propose a number of Ray core improvements to reduce the total amount of metadata needed during shuffle operations.
These improvements center around reducing the per-`ObjectRef` metadata needed.
Currently, each task requires about 5KB of metadata and each object requires about 2KB of metadata at the driver.
We plan to reduce these by:

1. "Collapsing" `ObjectRef` metadata for objects returned by the same task. All shuffle-on-Ray implementations rely on tasks with multiple return values. Currently, the metadata for these objects is stored separately, but we can combine these to amortize the metadata cost.
2. "Collapsing" task metadata for tasks submitted in the same stage. This is analogous to the above, but for tasks that are "similar".
3. Optimizing per-task and per-object metadata. Many of the metadata fields are not actually set or often have the same values. We can potentially save memory by not including these fields.

#### Fault tolerance and scheduling support

The push-based shuffle implemented in Exoshuffle currently requires precise placement of each task to reduce data movement across the cluster.
Currently, this is implemented using node-specific resource requirements.
However, this will hang the job if one of those nodes fails.

To ensure fault tolerance, we plan to implement soft scheduling constraints to allow tasks to execute even if their original node fails.
In the future, we can improve this further by providing a higher-level API to express scheduling constraints for groups of tasks.

## Compatibility, Deprecation, and Migration Plan

This proposal will not include any API changes other than a way to choose the shuffle implementation in Datasets.

## Test Plan and Acceptance Criteria
The proposal should discuss how the change will be tested **before** it can be merged or enabled. It should also include other acceptance criteria including documentation and examples. 

We plan to benchmark Datasets using the following workloads:
1. Shuffling data loader for ML ingest.
2. Sorting.
3. Groupby, using the [h2oai benchmark](https://h2oai.github.io/db-benchmark/).

We will also use chaos testing (random node failures) to check fault tolerance.

Initially, we will test the Datasets sort and compare performance both to Exoshuffle and the theoretical best (based on disk bandwidth specs).
We will also test scalability to confirm the current shuffle bottleneck in Datasets and determine the amount of driver memory needed to support a petabyte-scale sort.

Acceptance criteria:
* Shuffling data loader for ML ingest can scale to 100TB or more (requires push-based shuffle in Datasets).
* Ray Datasets sort can scale to petabyte-size data (requires Ray core optimizations).
* Ray includes a shuffle workload with millions of partitions in the nightly CI tests.

## (Optional) Follow-on Work

Some of the follow-up work includes:
* Further optimizations for groupby and other end-to-end Datasets benchmarks
* Improving scalability to larger clusters (100s or 1000 nodes)
* Providing a high-level API to express scheduling constraints between groups of tasks
* Further optimizations to reduce object overhead in Ray core (e.g., multi-part objects)
